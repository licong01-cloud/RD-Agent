"""
åˆ†æå› å­ä»»åŠ¡ä¸­è®­ç»ƒå®Œçš„æ¨¡å‹æƒé‡æ•°æ®ä½ç½®
"""
import pickle
from pathlib import Path, WindowsPath
import sys
import os
from collections import Counter

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, r"F:\Dev\RD-Agent-main")

# è‡ªå®šä¹‰Unpickleræ¥å¤„ç†è·¨å¹³å°è·¯å¾„é—®é¢˜
class CrossPlatformUnpickler(pickle.Unpickler):
    def find_class(self, module, name):
        if module == "pathlib" and name == "PosixPath":
            return WindowsPath
        return super().find_class(module, name)

# åˆ†æå› å­ä»»åŠ¡çš„æ¨¡å‹æƒé‡
def analyze_factor_model_weights(log_dir):
    """åˆ†æå› å­ä»»åŠ¡çš„æ¨¡å‹æƒé‡ä½ç½®"""
    session_folder = log_dir / "__session__"
    session_files = list(session_folder.rglob("1_coding"))
    
    if not session_files:
        return None
    
    latest_session = max(session_files, key=lambda p: p.stat().st_mtime)
    
    try:
        with open(latest_session, "rb") as f:
            session = CrossPlatformUnpickler(f).load()
        
        trace = session.trace
        
        # æ‰¾åˆ°SOTAå› å­å®éªŒ
        sota_factor_exp = None
        for exp, feedback in trace.hist:
            if "Factor" in type(exp).__name__ and feedback.decision:
                sota_factor_exp = exp
        
        if not sota_factor_exp:
            return None
        
        # è·å–workspaceè·¯å¾„
        workspace_path = str(sota_factor_exp.experiment_workspace.workspace_path)
        converted_path = workspace_path.replace("\\mnt\\f\\", "f:/").replace("\\", "/")
        
        # æ£€æŸ¥mlrunsç›®å½•
        mlruns_path = os.path.join(converted_path, "mlruns")
        
        model_files = []
        if os.path.exists(mlruns_path):
            # éå†mlrunsç›®å½•æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
            for root, dirs, files in os.walk(mlruns_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    # æŸ¥æ‰¾æ¨¡å‹ç›¸å…³æ–‡ä»¶
                    if any(keyword in file.lower() for keyword in ['model', 'lgb', 'booster', 'params', 'pkl']):
                        model_files.append({
                            'file': file,
                            'path': file_path,
                            'size': os.path.getsize(file_path) if os.path.isfile(file_path) else 0
                        })
        
        return {
            'log_dir': str(log_dir),
            'workspace_path': converted_path,
            'mlruns_path': mlruns_path,
            'mlruns_exists': os.path.exists(mlruns_path),
            'model_files': model_files,
            'model_count': len(model_files)
        }
    except Exception as e:
        return {
            'log_dir': str(log_dir),
            'error': str(e)
        }

# åˆ†æä¸€ä¸ªå…·ä½“çš„å› å­æ¼”è¿›ç›®å½•
log_root = Path(r"F:\Dev\RD-Agent-main\log")
log_dirs = [d for d in log_root.iterdir() if d.is_dir() and d.name != "__pycache__"]

# é€‰æ‹©ç¬¬ä¸€ä¸ªå› å­æ¼”è¿›ç›®å½•
target_log = Path(r"F:\Dev\RD-Agent-main\log\2025-12-18_10-38-22-336632")

print("=" * 80)
print("åˆ†æå› å­ä»»åŠ¡çš„æ¨¡å‹æƒé‡æ•°æ®")
print("=" * 80)
print(f"\nç›®æ ‡ç›®å½•: {target_log}")

result = analyze_factor_model_weights(target_log)

if result:
    print(f"\nWorkspaceè·¯å¾„: {result['workspace_path']}")
    print(f"MLrunsè·¯å¾„: {result['mlruns_path']}")
    print(f"MLrunså­˜åœ¨: {result['mlruns_exists']}")
    print(f"æ¨¡å‹æ–‡ä»¶æ•°é‡: {result['model_count']}")
    
    if result['model_files']:
        print(f"\næ¨¡å‹æ–‡ä»¶åˆ—è¡¨:")
        for i, mf in enumerate(result['model_files'], 1):
            print(f"\n  [{i}] {mf['file']}")
            print(f"      è·¯å¾„: {mf['path']}")
            print(f"      å¤§å°: {mf['size']} bytes")
            
            # å¦‚æœæ˜¯æ¨¡å‹æ–‡ä»¶ï¼Œå°è¯•è¯»å–è¯¦ç»†ä¿¡æ¯
            if mf['file'].endswith('.pkl'):
                try:
                    import pickle
                    with open(mf['path'], 'rb') as f:
                        obj = pickle.load(f)
                    print(f"      ç±»å‹: {type(obj).__name__}")
                    if hasattr(obj, '__dict__'):
                        print(f"      å±æ€§: {list(obj.__dict__.keys())}")
                except Exception as e:
                    print(f"      è¯»å–å¤±è´¥: {e}")
    else:
        print(f"\næœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        
        # æ£€æŸ¥artifactsç›®å½•ç»“æ„
        if result['mlruns_exists']:
            print(f"\nMLrunsç›®å½•ç»“æ„:")
            for root, dirs, files in os.walk(result['mlruns_path']):
                level = root.replace(result['mlruns_path'], '').count(os.sep)
                indent = ' ' * 2 * level
                print(f"{indent}{os.path.basename(root)}/")
                subindent = ' ' * 2 * (level + 1)
                for file in files[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªæ–‡ä»¶
                    print(f"{subindent}{file}")
                if len(files) > 5:
                    print(f"{subindent}... è¿˜æœ‰{len(files)-5}ä¸ªæ–‡ä»¶")
                if level > 2:  # é™åˆ¶æ·±åº¦
                    dirs[:] = []
else:
    print(f"åˆ†æå¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

# è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªå› å­å®éªŒçš„workspace
print("\n" + "=" * 80)
print("è¯¦ç»†åˆ†æSOTAå› å­å®éªŒçš„Workspace")
print("=" * 80)

session_folder = target_log / "__session__"
session_files = list(session_folder.rglob("1_coding"))
latest_session = max(session_files, key=lambda p: p.stat().st_mtime)

with open(latest_session, "rb") as f:
    session = CrossPlatformUnpickler(f).load()

trace = session.trace

# æ‰¾åˆ°SOTAå› å­å®éªŒ
for exp, feedback in trace.hist:
    if "Factor" in type(exp).__name__ and feedback.decision:
        print(f"\nSOTAå› å­å®éªŒ:")
        print(f"  å®éªŒç±»å‹: {type(exp).__name__}")
        print(f"  å†³ç­–: {feedback.decision}")
        
        workspace_path = str(exp.experiment_workspace.workspace_path)
        converted_path = workspace_path.replace("\\mnt\\f\\", "f:/").replace("\\", "/")
        
        print(f"  Workspaceè·¯å¾„: {converted_path}")
        
        # æ£€æŸ¥workspaceå†…å®¹
        if os.path.exists(converted_path):
            print(f"\n  Workspaceç›®å½•å†…å®¹:")
            items = os.listdir(converted_path)
            for item in sorted(items):
                item_path = os.path.join(converted_path, item)
                if os.path.isdir(item_path):
                    print(f"    ğŸ“ {item}/")
                else:
                    size = os.path.getsize(item_path)
                    print(f"    ğŸ“„ {item} ({size} bytes)")
            
            # æ£€æŸ¥mlrunsç›®å½•
            mlruns_path = os.path.join(converted_path, "mlruns")
            if os.path.exists(mlruns_path):
                print(f"\n  MLrunsç›®å½•åˆ†æ:")
                
                # æŸ¥æ‰¾experimentå’Œrun
                exp_dirs = [d for d in os.listdir(mlruns_path) if os.path.isdir(os.path.join(mlruns_path, d))]
                print(f"    Experiments: {len(exp_dirs)}")
                
                for exp_dir in exp_dirs[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    exp_path = os.path.join(mlruns_path, exp_dir)
                    print(f"\n    Experiment: {exp_dir}")
                    
                    run_dirs = [d for d in os.listdir(exp_path) if os.path.isdir(os.path.join(exp_path, d))]
                    print(f"      Runs: {len(run_dirs)}")
                    
                    for run_dir in run_dirs[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
                        run_path = os.path.join(exp_path, run_dir)
                        print(f"\n      Run: {run_dir}")
                        
                        # æ£€æŸ¥artifacts
                        artifacts_path = os.path.join(run_path, "artifacts")
                        if os.path.exists(artifacts_path):
                            print(f"        Artifacts:")
                            artifact_files = os.listdir(artifacts_path)
                            for af in artifact_files:
                                af_path = os.path.join(artifacts_path, af)
                                if os.path.isfile(af_path):
                                    size = os.path.getsize(af_path)
                                    print(f"          ğŸ“„ {af} ({size} bytes)")
                                else:
                                    print(f"          ğŸ“ {af}/")
                        
                        # æ£€æŸ¥metaæ–‡ä»¶
                        meta_path = os.path.join(run_path, "meta.yaml")
                        if os.path.exists(meta_path):
                            print(f"        Metaæ–‡ä»¶å­˜åœ¨")
                            try:
                                import yaml
                                with open(meta_path, 'r', encoding='utf-8') as f:
                                    meta = yaml.safe_load(f)
                                if 'artifact_uri' in meta:
                                    print(f"          Artifact URI: {meta['artifact_uri']}")
                            except Exception as e:
                                pass
        break

print("\n" + "=" * 80)
print("ç»“è®º")
print("=" * 80)
print("""
1. å› å­ä»»åŠ¡çš„æ¨¡å‹æƒé‡æ•°æ®ä½ç½®ï¼š
   - å­˜å‚¨åœ¨workspace/mlruns/{experiment_id}/{run_id}/artifacts/ç›®å½•
   - ä¸»è¦æ–‡ä»¶åŒ…æ‹¬ï¼š
     * model.pkl æˆ– modelæ–‡ä»¶ï¼šLightGBMæ¨¡å‹å¯¹è±¡
     * params.pklï¼šæ¨¡å‹å‚æ•°
     * pred.pklï¼šé¢„æµ‹ç»“æœ
     * ic.pklï¼šICåˆ†æç»“æœ

2. ä»SOTAå› å­è·å–æ¨¡å‹çš„æ–¹æ³•ï¼š
   - ä»session.trace.histæ‰¾åˆ°feedback.decision=Trueçš„å› å­å®éªŒ
   - è·å–è¯¥å®éªŒçš„experiment_workspace.workspace_path
   - åœ¨workspace/mlrunsç›®å½•ä¸‹æŸ¥æ‰¾æœ€æ–°çš„run
   - ä»artifactsç›®å½•è·å–æ¨¡å‹æ–‡ä»¶

3. ç”¨äºå®ç›˜é€‰è‚¡ï¼š
   - åŠ è½½æ¨¡å‹æ–‡ä»¶ï¼ˆmodel.pklï¼‰
   - åŠ è½½å› å­æ•°æ®
   - ä½¿ç”¨æ¨¡å‹é¢„æµ‹è‚¡ç¥¨æ”¶ç›Šç‡
   - æ ¹æ®é¢„æµ‹åˆ†æ•°è¿›è¡ŒTopKé€‰è‚¡
   - ç­‰æƒé‡ä¹°å…¥

4. æ¨¡å‹å¤ç”¨ï¼š
   - æ¨¡å‹æ–‡ä»¶å¯ä»¥ç›´æ¥åŠ è½½ä½¿ç”¨
   - éœ€è¦ç¡®ä¿å› å­æ•°æ®æ ¼å¼ä¸€è‡´
   - éœ€è¦å®šæœŸé‡æ–°è®­ç»ƒä»¥é€‚åº”å¸‚åœºå˜åŒ–
""")
