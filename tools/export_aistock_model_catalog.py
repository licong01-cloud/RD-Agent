import argparse
import json
import sqlite3
import sys
import uuid
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any

import yaml

# Add project root to sys.path to allow running as a script
project_root = str(Path(__file__).resolve().parents[1])
if project_root not in sys.path:
    sys.path.insert(0, project_root)


@dataclass
class ModelSourceRow:
    task_run_id: str
    loop_id: int
    workspace_id: str
    workspace_path: str


_MODEL_NS = uuid.UUID("a8a6a7de-c8c7-4f3e-9f5a-3c4b7d3e2b11")


def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def _load_yaml_if_exists(path: Path) -> dict[str, Any] | None:
    if not path.exists():
        return None
    try:
        with path.open("r", encoding="utf-8") as f:
            data = yaml.safe_load(f)
        return data if isinstance(data, dict) else None
    except Exception:
        return None


def _load_json_if_exists(path: Path) -> dict[str, Any] | None:
    if not path.exists():
        return None
    try:
        with path.open("r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def _fetch_model_sources(conn: sqlite3.Connection) -> list[dict[str, Any]]:
    """Fetch candidate models and workspaces for model catalog export.

    Strategy:
    - Join loops, workspaces, and artifacts.
    - Extract model metadata from artifacts table if present.
    """

    sql = """
    SELECT
      l.task_run_id,
      l.loop_id,
      l.log_dir,
      w.workspace_id,
      w.workspace_path,
      a.model_type,
      a.model_conf_json,
      a.dataset_conf_json,
      a.feature_schema_json
    FROM loops l
    JOIN workspaces w
      ON l.task_run_id = w.task_run_id
     AND l.loop_id = w.loop_id
    LEFT JOIN artifacts a
      ON l.task_run_id = a.task_run_id
     AND l.loop_id = a.loop_id
     AND w.workspace_id = a.workspace_id
     AND a.artifact_type = 'report'
    WHERE
      (l.has_result = 1 OR l.has_result = '1')
      AND w.workspace_path IS NOT NULL
      AND w.workspace_path != ''
    """
    rows = conn.execute(sql).fetchall()
    out: list[dict[str, Any]] = []
    for r in rows:
        out.append(dict(r))
    return out


def _extract_model_struct(ws_root: Path, registry_row: dict[str, Any]) -> dict[str, Any] | None:
    """Best-effort extract model info, prioritizing model.py, then model weight files, then YAML configs.
    
    REQ-MODEL-P3-010: Prioritize model.py for accurate model type extraction.
    REQ-MODEL-P3-020: Extract model type from model weight files if model.py not available.
    """

    if not ws_root.exists():
        return None

    model_config: dict[str, Any] | None = None
    dataset_config: dict[str, Any] | None = None
    feature_schema: Any | None = None
    model_type: str | None = None

    # 1. Extract actual model type from model.py (highest priority)
    model_py = ws_root / "model.py"
    if model_py.exists():
        try:
            with open(model_py, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # 查找模型类定义
            for line in content.split('\n'):
                line = line.strip()
                if line.startswith('class ') and '(' in line and 'Model' in line:
                    # 提取类名
                    class_start = line.find('class ') + 6
                    class_end = line.find('(')
                    if class_start > 5 and class_end > class_start:
                        model_type = line[class_start:class_end].strip()
                        break
                elif line.startswith('model_cls = '):
                    # 提取model_cls变量
                    model_type = line.split('=')[1].strip()
                    break
        except Exception:
            pass

    # 2. Fallback to model_meta.json (generated by Phase 3 backfill)
    if model_type is None:
        model_meta = _load_json_if_exists(ws_root / "model_meta.json")
        if model_meta:
            model_type = model_meta.get("model_type")
            model_config = model_meta.get("model_conf")
            dataset_config = model_meta.get("dataset_conf")
            feature_schema = model_meta.get("feature_schema")

    # 3. Fallback to model weight files (params.pkl, etc.)
    if model_type is None:
        mlruns_dir = ws_root / "mlruns"
        if mlruns_dir.exists():
            # 尝试从mlruns目录中提取模型类型信息
            for p in mlruns_dir.rglob("params.pkl"):
                try:
                    import pickle
                    with open(p, 'rb') as f:
                        params = pickle.load(f)
                    if isinstance(params, dict) and "model_type" in params:
                        model_type = params["model_type"]
                        break
                except Exception:
                    pass

    # 4. Fallback to Registry Metadata if still incomplete
    if model_config is None or dataset_config is None:
        if registry_row.get("model_type") and model_type is None:
            model_type = registry_row["model_type"]
        
        if registry_row.get("model_conf_json") and model_config is None:
            try:
                model_config = json.loads(registry_row["model_conf_json"])
            except Exception:
                pass
        if registry_row.get("dataset_conf_json") and dataset_config is None:
            try:
                dataset_config = json.loads(registry_row["dataset_conf_json"])
            except Exception:
                pass
        if registry_row.get("feature_schema_json") and feature_schema is None:
            try:
                feature_schema = json.loads(registry_row["feature_schema_json"])
            except Exception:
                pass

    # 5. Fallback to scanning workspace YAMLs if still incomplete
    if model_config is None or dataset_config is None or feature_schema is None:
        for p in list(ws_root.rglob("*.yaml")) + list(ws_root.rglob("*.yml")):
            cfg = _load_yaml_if_exists(p)
            if not cfg:
                continue
            
            # 支持顶层嵌套 scenario 的结构 (例如有些配置在 'scenario' 下)
            target_configs = [cfg]
            if "scenario" in cfg and isinstance(cfg["scenario"], dict):
                # 如果有 scenario，把 scenario 内的内容也作为搜索目标
                target_configs.append(cfg["scenario"])
            
            for base in target_configs:
                task_cfg = base.get("task")
                if isinstance(task_cfg, dict):
                    if model_config is None and isinstance(task_cfg.get("model"), dict):
                        model_config = task_cfg["model"]
                        if model_type is None and "class" in model_config:
                            model_type = model_config["class"]
                    if dataset_config is None and isinstance(task_cfg.get("dataset"), dict):
                        dataset_config = task_cfg["dataset"]
                
                # 尝试多种 Qlib 风格的 feature schema 位置
                dh_conf = base.get("data_handler_config")
                if isinstance(dh_conf, dict) and feature_schema is None:
                    feature_schema = dh_conf.get("feature")
                
                # 有些配置可能直接在顶级或者 task 级下有 data_handler_config
                if feature_schema is None and isinstance(task_cfg, dict):
                    dh_inner = task_cfg.get("dataset", {}).get("kwargs", {}).get("handler", {}).get("kwargs", {}).get("data_handler_config")
                    if isinstance(dh_inner, dict):
                        feature_schema = dh_inner.get("feature")

            if model_config is not None and dataset_config is not None and feature_schema is not None:
                break

    # 6. Model artifacts hints
    mlruns_dir = ws_root / "mlruns"
    model_files: list[str] = []
    # common model filenames
    for name in ("model.pkl", "model.joblib", "model.bin", "model.onnx", "model.pt", "model.pth"):
        p = ws_root / name
        if p.exists():
            model_files.append(p.name)

    if (model_config is None and dataset_config is None and feature_schema is None and 
        not model_files and not mlruns_dir.exists()):
        return None

    out: dict[str, Any] = {}
    if model_type:
        out["model_type"] = model_type
    if model_config is not None:
        out["model_config"] = model_config
    if dataset_config is not None:
        out["dataset_config"] = dataset_config
    if feature_schema is not None:
        out["feature_schema"] = feature_schema
        
        # Phase 3 Enhancement: Flatten feature list if possible
        # Qlib feature schema can be: 
        # 1) List of expressions: ["$close/Ref($close,1)-1", ...]
        # 2) Tuple: (["$close/Ref($close,1)-1"], ["factor_name"])
        # 3) Dict: {"feature": [...], "label": [...]}
        flattened_features = []
        if isinstance(feature_schema, list):
            flattened_features = feature_schema
        elif isinstance(feature_schema, tuple) and len(feature_schema) >= 1:
            if isinstance(feature_schema[0], list):
                flattened_features = feature_schema[0]
        elif isinstance(feature_schema, dict):
            # some loaders put them under 'feature' or 'fields'
            for k in ("feature", "fields", "columns"):
                if k in feature_schema and isinstance(feature_schema[k], list):
                    flattened_features = feature_schema[k]
                    break
        
        if flattened_features:
            out["flattened_feature_list"] = flattened_features

    artifacts: dict[str, Any] = {}
    if mlruns_dir.exists():
        artifacts["mlruns"] = "mlruns"
    if model_files:
        artifacts["model_files"] = model_files
    if artifacts:
        out["model_artifacts"] = artifacts

    return out


def _to_native_path(p_str: str) -> Path:
    """Convert path between WSL and Windows format based on current OS."""
    import os
    if not p_str:
        return Path()
    is_windows = os.name == "nt"
    if is_windows and p_str.startswith("/mnt/"):
        parts = p_str.split("/")
        if len(parts) < 3:
            return Path(p_str)
        drive = parts[2].upper()
        return Path(f"{drive}:\\") / Path(*parts[3:])
    elif not is_windows and len(p_str) > 1 and p_str[1] == ":" and p_str[2] == "\\":
        drive = p_str[0].lower()
        rel = p_str[3:].replace("\\", "/")
        return Path(f"/mnt/{drive}") / rel
    return Path(p_str)


def run(registry_sqlite: Path, output_path: Path) -> None:
    conn = sqlite3.connect(str(registry_sqlite))
    conn.row_factory = sqlite3.Row
    try:
        sources = _fetch_model_sources(conn)
    finally:
        conn.close()

    models: list[dict[str, Any]] = []

    for src in sources:
        ws_root = _to_native_path(src["workspace_path"])
        struct = _extract_model_struct(ws_root, src)
        if not struct:
            continue

        # 基本标识字段
        entry: dict[str, Any] = {
            "task_run_id": src["task_run_id"],
            "loop_id": src["loop_id"],
            "log_dir": src.get("log_dir"),
            "workspace_id": src["workspace_id"],
            "workspace_path": src["workspace_path"],
        }
        entry.update(struct)

        # 基于实例配置生成稳定的 model_id（模型实例粒度）
        model_id_base = {
            "task_run_id": entry["task_run_id"],
            "loop_id": entry["loop_id"],
            "workspace_id": entry["workspace_id"],
            "model_type": entry.get("model_type"),
            "model_config": entry.get("model_config"),
            "dataset_config": entry.get("dataset_config"),
        }
        model_id_key = json.dumps(model_id_base, sort_keys=True, ensure_ascii=False, default=str)
        entry["model_id"] = uuid.uuid5(_MODEL_NS, model_id_key).hex

        models.append(entry)

    payload = {
        "version": "v1",
        "generated_at_utc": _utc_now_iso(),
        "source": "rdagent_tools",
        "models": models,
    }

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2, default=str), encoding="utf-8")


def main() -> None:
    parser = argparse.ArgumentParser(description="Export AIstock-facing model catalog as JSON.")
    parser.add_argument(
        "--registry-sqlite",
        required=True,
        help="Path to registry.sqlite (WSL/Linux path).",
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Output JSON path for model_catalog.",
    )
    args = parser.parse_args()

    registry_sqlite = Path(args.registry_sqlite)
    if not registry_sqlite.exists():
        raise SystemExit(f"registry.sqlite not found: {registry_sqlite}")

    output_path = Path(args.output)
    run(registry_sqlite=registry_sqlite, output_path=output_path)


if __name__ == "__main__":
    main()
