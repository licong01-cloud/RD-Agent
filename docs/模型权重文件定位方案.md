# 模型权重文件定位方案

**文档版本**: v1.0  
**生成时间**: 2026-01-16  
**适用场景**: AIstock侧执行实盘数据选股

---

## 一、问题背景

在RD-Agent任务结束后，需要在AIstock侧执行实盘数据选股。为此，需要：

1. **获取SOTA因子列表**：所有被接受的因子（`decision=True`）
2. **获取模型权重文件**：训练好的模型权重，包含SOTA因子和alpha22个因子的入库
3. **获取因子数据文件**：`combined_factors_df.parquet`，包含所有因子的计算结果
4. **在AIstock侧使用**：用SOTA和alpha组合的因子和训练好的模型，执行实盘数据选股

---

## 二、核心机制分析

### 2.1 SOTA因子与模型的关联机制

根据`SOTA因子完整分析文档_v2.md`的分析：

```
Session文件结构：
log/YYYY-MM-DD_HH-MM-SS-XXXXXX/__session__/0/1_coding
  └── trace.hist: [(Experiment0, Feedback0), (Experiment1, Feedback1), ...]
```

**关键发现**：

1. **SOTA因子累积机制**：
   - 所有`feedback.decision=True`的因子实验都会保留在`based_experiments`中
   - SOTA因子会累积，所有被接受的因子都会保留

2. **SOTA模型替换机制**：
   - 只保留最新的一个模型
   - 新的`decision=True`的模型会替换之前的SOTA模型

3. **模型权重文件存储位置**：
   - **位置1**：`exp.sub_workspace_list[0].file_dict['model.pkl']`（session文件中）
   - **位置2**：`workspace_path/model.pkl`（workspace目录中）
   - **位置3**：`workspace_path/mlruns/**/params.pkl`（MLflow记录）

### 2.2 因子与模型的关联

**关键代码位置**：`rdagent/scenarios/qlib/developer/model_runner.py:195-210`

```python
# 保存合并的因子数据
target_path = exp.experiment_workspace.workspace_path / "combined_factors_df.parquet"
combined_factors.to_parquet(target_path, engine="pyarrow")
```

**关联机制**：

1. **因子合并**：
   - SOTA因子 + 新因子 → `combined_factors_df.parquet`
   - 包含所有因子的计算结果

2. **模型训练**：
   - 使用`combined_factors_df.parquet`作为输入
   - 训练模型，生成`model.pkl`

3. **权重文件**：
   - `model.pkl`包含训练好的模型权重
   - 模型会学习每个因子的权重（用于选股评分）

---

## 三、模型权重文件定位方案

### 方案1：从Session文件中获取（推荐）

#### 3.1.1 定位步骤

```
步骤1: 加载Session文件
  路径: log/{task_id}/__session__/0/1_coding
  格式: Pickle

步骤2: 获取最新SOTA模型
  遍历: reversed(session.trace.hist)
  条件: feedback.decision=True AND 'Model' in type(exp).__name__
  
步骤3: 提取模型权重文件
  位置: exp.sub_workspace_list[0].file_dict['model.pkl']
  格式: bytes

步骤4: 获取combined_factors_df.parquet路径
  位置: exp.experiment_workspace.workspace_path / "combined_factors_df.parquet"
  格式: Path
```

#### 3.1.2 Python代码示例

```python
import pickle
from pathlib import Path

def extract_model_weight_from_session(task_log_path: Path, output_path: Path = None):
    """
    从session文件中提取模型权重文件
    
    Args:
        task_log_path: log目录路径，如 log/2026-01-15_16-06-55-484508
        output_path: 输出文件路径，默认为model_weights.pkl
    
    Returns:
        dict: 包含模型权重文件和相关信息的字典
    """
    if output_path is None:
        output_path = Path("model_weights.pkl")
    
    # 步骤1: 加载session文件
    session_file = task_log_path / "__session__" / "0" / "1_coding"
    with open(session_file, 'rb') as f:
        session = pickle.load(f)
    
    # 步骤2: 获取最新SOTA模型
    sota_model = None
    for exp, feedback in reversed(session.trace.hist):
        if feedback and feedback.decision:
            exp_type = type(exp).__name__
            if 'Model' in exp_type:
                sota_model = exp
                break
    
    if not sota_model:
        raise ValueError("未找到SOTA模型")
    
    # 步骤3: 提取模型权重文件
    model_weights = None
    model_weight_filename = None
    
    if hasattr(sota_model, 'sub_workspace_list'):
        for sw in sota_model.sub_workspace_list:
            if sw and hasattr(sw, 'file_dict'):
                # 查找权重文件（优先model.pkl，其次params.pkl）
                for filename in ['model.pkl', 'params.pkl']:
                    if filename in sw.file_dict:
                        model_weights = sw.file_dict[filename]
                        model_weight_filename = filename
                        break
                if model_weights:
                    break
    
    if not model_weights:
        raise ValueError("未找到模型权重文件")
    
    # 保存到文件
    with open(output_path, 'wb') as f:
        f.write(model_weights)
    
    # 步骤4: 获取combined_factors_df.parquet路径
    workspace_path = None
    combined_factors_path = None
    
    if hasattr(sota_model, 'experiment_workspace'):
        workspace_path = sota_model.experiment_workspace.workspace_path
        combined_factors_path = workspace_path / "combined_factors_df.parquet"
    
    return {
        'model_weight_file': str(output_path),
        'model_weight_filename': model_weight_filename,
        'model_weight_size': len(model_weights),
        'workspace_path': str(workspace_path) if workspace_path else None,
        'combined_factors_path': str(combined_factors_path) if combined_factors_path and combined_factors_path.exists() else None,
    }

# 使用示例
task_log_path = Path(r"log\2026-01-15_16-06-55-484508")
result = extract_model_weight_from_session(task_log_path, Path("model_weights.pkl"))
print(result)
```

#### 3.1.3 优点

- ✅ 不依赖workspace目录
- ✅ 数据完整且可靠
- ✅ 适合外部系统集成
- ✅ 可以同时获取SOTA因子列表和模型权重

#### 3.1.4 缺点

- ❌ 需要加载整个session文件（可能较大）
- ❌ 依赖rdagent模块（需要安装）

---

### 方案2：从Workspace目录获取

#### 3.2.1 定位步骤

```
步骤1: 加载Session文件，获取最新SOTA模型
  同方案1步骤1-2

步骤2: 获取workspace路径
  位置: exp.experiment_workspace.workspace_path
  格式: Path

步骤3: 在workspace目录中查找权重文件
  查找: workspace_path/model.pkl 或 workspace_path/params.pkl
  格式: 文件路径

步骤4: 复制权重文件
  操作: shutil.copy2(source, target)
```

#### 3.2.2 Python代码示例

```python
import pickle
import shutil
from pathlib import Path

def extract_model_weight_from_workspace(task_log_path: Path, output_path: Path = None):
    """
    从workspace目录中提取模型权重文件
    
    Args:
        task_log_path: log目录路径
        output_path: 输出文件路径
    
    Returns:
        dict: 包含模型权重文件和相关信息的字典
    """
    if output_path is None:
        output_path = Path("model_weights.pkl")
    
    # 步骤1-2: 加载session，获取SOTA模型
    session_file = task_log_path / "__session__" / "0" / "1_coding"
    with open(session_file, 'rb') as f:
        session = pickle.load(f)
    
    sota_model = None
    for exp, feedback in reversed(session.trace.hist):
        if feedback and feedback.decision:
            exp_type = type(exp).__name__
            if 'Model' in exp_type:
                sota_model = exp
                break
    
    if not sota_model:
        raise ValueError("未找到SOTA模型")
    
    # 步骤3: 获取workspace路径
    if not hasattr(sota_model, 'experiment_workspace'):
        raise ValueError("SOTA模型没有experiment_workspace")
    
    workspace_path = sota_model.experiment_workspace.workspace_path
    
    # 查找权重文件
    weight_file = None
    for filename in ['model.pkl', 'params.pkl']:
        candidate = workspace_path / filename
        if candidate.exists():
            weight_file = candidate
            break
    
    if not weight_file:
        raise ValueError(f"workspace中未找到权重文件: {workspace_path}")
    
    # 步骤4: 复制权重文件
    shutil.copy2(weight_file, output_path)
    
    return {
        'model_weight_file': str(output_path),
        'model_weight_filename': weight_file.name,
        'model_weight_size': weight_file.stat().st_size,
        'workspace_path': str(workspace_path),
        'combined_factors_path': str(workspace_path / "combined_factors_df.parquet"),
    }

# 使用示例
task_log_path = Path(r"log\2026-01-15_16-06-55-484508")
result = extract_model_weight_from_workspace(task_log_path, Path("model_weights.pkl"))
print(result)
```

#### 3.2.3 优点

- ✅ 可以直接访问文件
- ✅ 适合本地调试
- ✅ 不需要处理pickle对象

#### 3.2.4 缺点

- ❌ 依赖workspace目录
- ❌ workspace可能被清理
- ❌ 仍需要加载session文件获取workspace路径

---

### 方案3：直接遍历Workspace目录（备用）

#### 3.3.1 定位步骤

```
步骤1: 遍历git_ignore_folder/RD-Agent_workspace/
  查找: 所有workspace目录

步骤2: 在每个workspace中查找权重文件
  查找: model.pkl 或 params.pkl

步骤3: 根据修改时间选择最新的权重文件
  排序: 按文件修改时间降序
  选择: 最新的文件

步骤4: 复制权重文件
  操作: shutil.copy2(source, target)
```

#### 3.3.2 Python代码示例

```python
import shutil
from pathlib import Path
from datetime import datetime

def find_latest_model_weight(workspace_root: Path, output_path: Path = None):
    """
    在workspace目录中查找最新的模型权重文件
    
    Args:
        workspace_root: workspace根目录，如 git_ignore_folder/RD-Agent_workspace
        output_path: 输出文件路径
    
    Returns:
        dict: 包含模型权重文件信息的字典
    """
    if output_path is None:
        output_path = Path("model_weights.pkl")
    
    # 查找所有权重文件
    weight_files = []
    
    for ws_dir in workspace_root.iterdir():
        if not ws_dir.is_dir():
            continue
        
        # 查找model.pkl
        model_pkl = ws_dir / "model.pkl"
        if model_pkl.exists():
            weight_files.append(model_pkl)
            continue
        
        # 查找params.pkl（在mlruns目录中）
        for mlruns_file in ws_dir.rglob("params.pkl"):
            if "mlruns" in mlruns_file.parts:
                weight_files.append(mlruns_file)
    
    if not weight_files:
        raise ValueError(f"未找到模型权重文件: {workspace_root}")
    
    # 按修改时间排序，选择最新的
    weight_files.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    latest_weight_file = weight_files[0]
    
    # 复制权重文件
    shutil.copy2(latest_weight_file, output_path)
    
    return {
        'model_weight_file': str(output_path),
        'model_weight_filename': latest_weight_file.name,
        'model_weight_size': latest_weight_file.stat().st_size,
        'workspace_path': str(latest_weight_file.parent),
        'modification_time': datetime.fromtimestamp(latest_weight_file.stat().st_mtime),
    }

# 使用示例
workspace_root = Path(r"git_ignore_folder\RD-Agent_workspace")
result = find_latest_model_weight(workspace_root, Path("model_weights.pkl"))
print(result)
```

#### 3.3.3 优点

- ✅ 不需要加载session文件
- ✅ 不依赖rdagent模块
- ✅ 适合批量处理

#### 3.3.4 缺点

- ❌ 需要遍历所有workspace目录
- ❌ 可能找到历史文件
- ❌ 无法确定是哪个任务的模型

---

## 四、推荐方案：方案1（从Session文件获取）

### 4.1 推荐理由

1. **数据完整可靠**：session文件包含完整的实验历史
2. **不依赖外部目录**：workspace可能被清理
3. **适合AIstock系统集成**：可以同时获取SOTA因子列表和模型权重
4. **可以获取关联信息**：workspace路径、combined_factors_df.parquet路径

### 4.2 完整流程

```
1. 加载session文件
   ├── 读取: log/{task_id}/__session__/0/1_coding
   └── 解析: pickle.load()

2. 提取SOTA因子列表
   ├── 遍历: session.trace.hist
   ├── 筛选: feedback.decision=True AND 'Factor' in type(exp).__name__
   └── 提取: 因子名称、表达式、代码、性能指标

3. 提取最新SOTA模型
   ├── 遍历: reversed(session.trace.hist)
   ├── 筛选: feedback.decision=True AND 'Model' in type(exp).__name__
   └── 提取: 模型代码、模型权重、性能指标

4. 提取模型权重文件
   ├── 位置: exp.sub_workspace_list[0].file_dict['model.pkl']
   └── 保存: model_weights.pkl

5. 提取combined_factors_df.parquet路径
   ├── 位置: exp.experiment_workspace.workspace_path / "combined_factors_df.parquet"
   └── 读取: 因子列表、因子数据

6. 提供给AIstock
   ├── SOTA因子列表
   ├── 模型权重文件
   ├── 模型代码
   ├── 因子代码
   └── combined_factors_df.parquet
```

### 4.3 AIstock集成示例

```python
import pickle
import pandas as pd
from pathlib import Path

def extract_all_for_aistock(task_log_path: Path, output_dir: Path):
    """
    提取AIstock需要的所有数据
    
    Args:
        task_log_path: log目录路径
        output_dir: 输出目录
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. 加载session文件
    session_file = task_log_path / "__session__" / "0" / "1_coding"
    with open(session_file, 'rb') as f:
        session = pickle.load(f)
    
    # 2. 提取SOTA因子列表
    sota_factors = []
    for exp, feedback in session.trace.hist:
        if feedback and feedback.decision:
            exp_type = type(exp).__name__
            if 'Factor' in exp_type:
                factor_info = {
                    'name': exp.sub_tasks[0].factor_name,
                    'formulation': exp.sub_tasks[0].factor_formulation,
                    'code': exp.sub_workspace_list[0].file_dict.get('factor.py'),
                    'metrics': exp.result,
                    'reason': feedback.reason,
                }
                sota_factors.append(factor_info)
    
    # 保存SOTA因子列表
    with open(output_dir / 'sota_factors.json', 'w', encoding='utf-8') as f:
        import json
        json.dump(sota_factors, f, ensure_ascii=False, indent=2)
    
    # 3. 提取最新SOTA模型
    sota_model = None
    for exp, feedback in reversed(session.trace.hist):
        if feedback and feedback.decision:
            exp_type = type(exp).__name__
            if 'Model' in exp_type:
                sota_model = exp
                break
    
    if not sota_model:
        raise ValueError("未找到SOTA模型")
    
    # 4. 提取模型权重文件
    model_weights = None
    for sw in sota_model.sub_workspace_list:
        if sw and hasattr(sw, 'file_dict'):
            for filename in ['model.pkl', 'params.pkl']:
                if filename in sw.file_dict:
                    model_weights = sw.file_dict[filename]
                    with open(output_dir / 'model_weights.pkl', 'wb') as f:
                        f.write(model_weights)
                    break
            if model_weights:
                break
    
    # 5. 提取模型代码
    model_code = sota_model.sub_workspace_list[0].file_dict.get('model.py')
    if model_code:
        with open(output_dir / 'model.py', 'w', encoding='utf-8') as f:
            f.write(model_code)
    
    # 6. 提取combined_factors_df.parquet
    workspace_path = sota_model.experiment_workspace.workspace_path
    combined_factors_path = workspace_path / "combined_factors_df.parquet"
    
    if combined_factors_path.exists():
        # 读取因子数据
        combined_factors = pd.read_parquet(combined_factors_path)
        
        # 保存因子列表
        factor_columns = list(combined_factors.columns)
        with open(output_dir / 'factor_columns.json', 'w', encoding='utf-8') as f:
            import json
            json.dump(factor_columns, f, ensure_ascii=False, indent=2)
        
        # 复制combined_factors_df.parquet
        import shutil
        shutil.copy2(combined_factors_path, output_dir / 'combined_factors_df.parquet')
    
    # 7. 保存模型性能指标
    model_metrics = sota_model.result
    with open(output_dir / 'model_metrics.json', 'w', encoding='utf-8') as f:
        import json
        json.dump(model_metrics, f, ensure_ascii=False, indent=2)
    
    return {
        'sota_factors_count': len(sota_factors),
        'model_weight_file': str(output_dir / 'model_weights.pkl'),
        'model_code_file': str(output_dir / 'model.py'),
        'combined_factors_file': str(output_dir / 'combined_factors_df.parquet'),
        'factor_columns_file': str(output_dir / 'factor_columns.json'),
    }

# 使用示例
task_log_path = Path(r"log\2026-01-15_16-06-55-484508")
output_dir = Path(r"aistock_data")
result = extract_all_for_aistock(task_log_path, output_dir)
print(result)
```

---

## 五、AIstock侧使用方案

### 5.1 数据准备

AIstock需要以下数据：

| 数据类型 | 文件名 | 说明 |
|---------|--------|------|
| **SOTA因子列表** | `sota_factors.json` | 所有被接受的因子信息 |
| **模型权重文件** | `model_weights.pkl` | 训练好的模型权重 |
| **模型代码** | `model.py` | 模型实现代码 |
| **因子代码** | `factor_*.py` | 每个因子的实现代码 |
| **因子数据文件** | `combined_factors_df.parquet` | 所有因子的计算结果 |
| **因子列表** | `factor_columns.json` | 因子列名列表 |
| **模型性能指标** | `model_metrics.json` | 模型回测性能指标 |

### 5.2 选股流程

```python
import pickle
import pandas as pd
import joblib
from pathlib import Path

def stock_selection_with_sota_model(data_dir: Path, market_data: pd.DataFrame):
    """
    使用SOTA模型进行选股
    
    Args:
        data_dir: 数据目录
        market_data: 市场数据
    
    Returns:
        选股结果
    """
    # 1. 加载模型权重
    with open(data_dir / 'model_weights.pkl', 'rb') as f:
        model = pickle.load(f)
    
    # 2. 加载因子列表
    with open(data_dir / 'factor_columns.json', 'r', encoding='utf-8') as f:
        import json
        factor_columns = json.load(f)
    
    # 3. 计算因子值
    # 这里需要根据因子代码计算每个因子的值
    # 可以参考RD-Agent的因子实现
    
    # 4. 合并因子数据
    factors_df = pd.DataFrame(index=market_data.index, columns=factor_columns)
    
    # 5. 模型预测
    predictions = model.predict(factors_df)
    
    # 6. 选股
    # 根据预测结果排序，选择top K只股票
    top_k = 50
    selected_stocks = predictions.nlargest(top_k)
    
    return selected_stocks
```

---

## 六、总结

### 6.1 方案对比

| 方案 | 优点 | 缺点 | 推荐度 |
|------|------|------|--------|
| **方案1: 从Session获取** | 数据完整、不依赖外部目录 | 需要加载session、依赖rdagent | ⭐⭐⭐⭐⭐ |
| **方案2: 从Workspace获取** | 可直接访问文件 | 依赖workspace、仍需session | ⭐⭐⭐ |
| **方案3: 遍历Workspace** | 不需session、不依赖rdagent | 可能找到历史文件 | ⭐⭐ |

### 6.2 推荐流程

```
1. 加载Session文件
   └── log/{task_id}/__session__/0/1_coding

2. 提取SOTA因子列表
   └── 所有decision=True的因子实验

3. 提取最新SOTA模型
   └── 倒序遍历，第一个decision=True的模型实验

4. 提取模型权重文件
   └── exp.sub_workspace_list[0].file_dict['model.pkl']

5. 提取combined_factors_df.parquet
   └── exp.experiment_workspace.workspace_path / "combined_factors_df.parquet"

6. 提供给AIstock
   └── SOTA因子列表 + 模型权重 + 因子数据
```

### 6.3 关键要点

1. **模型权重文件**：`model.pkl`包含训练好的模型权重，用于选股评分
2. **因子数据文件**：`combined_factors_df.parquet`包含所有因子的计算结果
3. **SOTA因子列表**：所有被接受的因子（`decision=True`）
4. **关联机制**：模型使用`combined_factors_df.parquet`训练，权重反映每个因子的重要性
5. **AIstock使用**：用SOTA因子计算实时因子值，用模型权重进行选股评分

---

## 七、附录

### 7.1 相关文档

- `SOTA因子完整分析文档_v2.md`：SOTA因子的完整分析
- `rdagent/scenarios/qlib/developer/model_runner.py`：模型运行器
- `rdagent/scenarios/qlib/developer/factor_runner.py`：因子运行器
- `rdagent/utils/solidification.py`：模型权重文件处理

### 7.2 关键代码位置

| 功能 | 文件位置 | 行号 |
|------|---------|------|
| **模型权重提取** | `rdagent/utils/solidification.py` | 331-355 |
| **combined_factors保存** | `rdagent/scenarios/qlib/developer/model_runner.py` | 195-210 |
| **SOTA模型选择** | `rdagent/scenarios/qlib/developer/factor_runner.py` | 316-319 |
| **Session结构** | `rdagent/core/proposal.py` | 139-141 |

### 7.3 常见问题

**Q1: 模型权重文件包含哪些信息？**

A: 模型权重文件包含训练好的模型参数，用于在选股阶段对每个因子进行加权评分。具体内容取决于模型类型（如MLP、LightGBM、GRU等）。

**Q2: 如何知道模型使用了哪些因子？**

A: 模型使用的因子列表存储在`combined_factors_df.parquet`的列名中，可以通过读取该文件获取。

**Q3: SOTA因子和alpha因子的区别？**

A: SOTA因子是RD-Agent自动生成并经过回测验证的因子，alpha因子是预定义的因子（如RESI5、WVMA5等）。模型会同时使用这两类因子。

**Q4: 如何在AIstock侧使用模型权重？**

A: 加载`model.pkl`文件，计算实时因子值，输入模型进行预测，根据预测结果选股。

**Q5: 如果workspace被清理了怎么办？**

A: 使用方案1（从Session文件获取），不依赖workspace目录。

---

**文档结束**
