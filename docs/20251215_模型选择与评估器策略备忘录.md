# 20251215 模型选择与评估器策略备忘录

## 目标

- 明确“回测评估是否必须使用 LGBM”与“如何让模型选择更开放”的结论与依据。
- 给出 RD-Agent 当前实现中的真实约束点（配置/分支/提示词注入），避免误把问题归因到 LLM。
- 给出可落地的后续改造路线（先验证回测准确性，再逐步放开评估器与模型选择）。

## 结论（先给结论）

- 回测评估并不必须使用 LGBM 才“有效”。有效性的前提是：数据输入正确、模型能产出预测、记录器能写出结果、读取结果的逻辑拿到的是本次 run 的结果。
- 目前多轮“看起来只在用 LGBM”的根因主要来自：Qlib YAML 模板（factor backtest）中 `task.model.class` 写死为 `LGBModel`，以及 runner 在这些模板间切换；而不是提示词对模型做了“必须 LGBM”的硬性限制。
- 在当前实现里，“因子研发（factor loop）”的评估器基本固定为 LGBM；“模型研发（model loop）”走的是注入 `model.py` 的 PyTorch 路线，不受 LGBM 限制，但受 `model_type`（Tabular/TimeSeries）与部分提示约束影响。

## 当前实现里的约束点梳理

### 1) 因子回测（factor backtest）评估器固定

- `conf_baseline.yaml` 与 `conf_combined_factors_dynamic.yaml`：`task.model.class: LGBModel`。
- `QlibFactorRunner.develop()`：选择 `conf_baseline.yaml` / `conf_combined_factors_dynamic.yaml` / `conf_combined_factors_sota_model.yaml` 运行回测。
- 因此：即便上层 LLM 提出“换模型”，只要执行链路走 factor backtest，这轮评估仍会固定用 LGBM。

### 2) 模型研发（model loop）是 PyTorch 模型注入

- `QlibModelRunner.develop()`：依赖 `exp.sub_tasks[0].model_type` 选择 Dataset 类型，并运行 `conf_*_factors_model.yaml`。
- 这一链路的“模型选择”更多体现在 `model.py` 的实现与 `model_type`，而不是 qlib YAML 中的 `LGBModel`。

### 3) “RAG”并非外部平台，而是内部提示注入槽位

- `quant_proposal.py` 中的 `qaunt_rag` 是一段硬编码文本，根据 action（factor/model）注入到 prompts。
- 它会影响 LLM 倾向（例如暂不生成 GNN），但不决定 factor backtest 的评估器。

## 业界最佳实践：评估器与研发目标分层

建议把“评估”拆成 3 层，以免把不同目标混在一个模型里：

### A. 因子层（弱模型/无模型）

- 指标：Rank IC/ICIR、分层收益、稳定性、换手、行业/风格暴露、交易成本敏感性。
- 目的：验证因子本身的 Alpha 属性与稳健性。
- 优点：解释性强，便于定位“因子真的在变好”还是“模型在拟合噪声”。

### B. 评估器层（统一、中等复杂度模型）

- 典型：LGBM/XGBoost、Ridge/ElasticNet。
- 目的：在固定策略与固定评估器下对比不同因子集合的边际增益。
- 最佳实践：至少提供一个线性 baseline（Ridge）+ 一个 GBDT（LGBM），形成对照。

### C. 策略层（交易规则与组合构建）

- 固定策略（topk/换手约束/成本/风控）用于对比。
- 少量 Top 候选再进入更重模型（时序 NN）与更严格 walk-forward 验证。

## 后续改造建议（先验证，再放开）

### Phase 0：先验证回测“是否准确”

- 目标：确认每一轮 backtest 确实吃到了“当轮生成的因子文件”，并且读取到的是“当轮 run 的结果”。
- 当前已存在的保护：在 `QlibFBWorkspace.execute()` 中拒绝包含 `static_path:` 的配置，避免静态因子回退。

### Phase 1：评估器可配置化（不让 LLM 直接决定）

- 目标：让“评估器选择”变成结构化配置（外部 JSON 或环境变量），而不是 prompt 自由发挥。
- 做法：
  - 新增多个 factor backtest 的 qlib YAML，例如 `conf_baseline_ridge.yaml`、`conf_combined_factors_ridge.yaml`。
  - runner 根据 `eval_model_family` 选择 yaml。
  - workspace 输出本轮使用的 `qlib_config_name` 与输入指纹（见 Phase 2）。

### Phase 2：可审计性（输入指纹与配置回写）

- 写入 workspace 或日志：
  - `qlib_config_name`
  - `provider_uri`
  - 关键输入文件（例如 `combined_factors_df.parquet`）的 size/mtime/sha1
  - 关键输出（`qlib_res.csv`、`ret.pkl`）的 mtime/sha1

### Phase 3：扩大模型研发空间（可选）

- 如果要让 LLM“更多发挥空间”，建议只在 model loop（PyTorch）放开：
  - 允许更多时序/表格结构，但用参数量/训练时长约束。
  - 将“禁 GNN”从硬禁令改为软约束（仅在无图数据支持时不建议）。

## 风险与注意事项

- 评估器一旦变成可选模型集合，必须保证“对比公平”：
  - 同一数据切分、同一 label、同一特征处理、同一策略与成本
  - 否则不同模型的差异会掩盖因子真实增益
- 重模型评估器（NN）容易引入训练不稳定与耗时，建议只用于少量候选复核。

## 建议的最小落地标准

- 保留 LGBM 作为主评估器（稳定、快速），并增加 Ridge 作为对照评估器。
- 将评估器选择与回测模板选择从“隐式默认”改为“显式可配置 + 可审计”。
